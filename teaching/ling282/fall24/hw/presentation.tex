\documentclass[11pt]{article}
\usepackage{hyperref}
\usepackage[margin=0.75in]{geometry}
\usepackage{amsmath}


\begin{document}

\title{Ling 282/482 Paper Presentation}
\date{\vspace{-0.2in}Presented in-class December 2 or 4, 2024}
\maketitle

\section*{Presentation Description and Requirements}
For this assignment, you will present a research paper during class on either December 2 or December 4 (depending on a randomized sign-up order). The paper will be chosen from among the options included later in this document. These papers all relate to the intersection of Deep Learning and Linguistics, with a particular emphasis on understanding Language or linguistic structure, rather than simply engineering a system that accomplishes a Natural Language Processing task.

During the course of this assignment, it is likely you will come across concepts (either Linguistic or Computational) with which you are not familiar; this is okay! Almost nobody understands everything written in a research paper they are reading for the first time. You can use any resources at your disposal (including the instructor) to clarify concepts that you are uncertain about.

Your presentation should last for 25 minutes with 10 minutes allocated for questions and discussion with the rest of the class. You can present with either slides or a paper hand-out, and turn in a copy of these to the instructor. You should address the following aspects of the paper:
\begin{itemize}
 \item Overall idea (research question(s), general approach, general finding)
 \item Background (what do we need to know to understand the paper?)
 \item Methodology (how do the authors go about answering the question?) This should particularly address how Deep Learning is used in or relates to the paper
 \item Findings (more detailed assessment of the results)
 \item Significance (what does this work tell us about Language and/or Deep Learning?)
 \item Critical review (do you think the authors' findings and/or usage of Deep Learning are sound?)
 \item One or two things that you are still uncertain/unclear about
 \item One or two discussion questions/topics for the class
\end{itemize}

\section*{Paper Options}
The following are pre-approved options for papers to present on, organized by general topic. If you are very interested in choosing a different paper, you must have it approved by the instructor no later than 10 days before your presentation. If choosing a paper from this list, email the instructor with your \textbf{first- and second-choice} papers no later than \textbf{Wednesday November 20}. Students will receive their paper of choice with priority that is the \textbf{reverse} of the presentation slot choice. So if you have last choice of presentation slot, you have first choice of paper.

Multilinguality/Translation
\begin{itemize}
 \item \href{https://arxiv.org/abs/2403.18031}{The Impact of Syntactic and Semantic Proximity on Machine Translation with Back-Translation}
 \item \href{https://aclanthology.org/2021.findings-emnlp.382/}{A multilabel approach to morphosyntactic probing}
 \item \href{https://aclanthology.org/2024.emnlp-main.326/}{What is ``Typological Diversity'' in NLP?}
 \item \href{https://aclanthology.org/2023.findings-eacl.89/}{Multilingual BERT has an accent}
 \item \href{https://aclanthology.org/P19-1301/}{Choosing Transfer Learning for Cross-Lingual Learning}
\end{itemize}

Cognition / Formal Languages
\begin{itemize}
  \item \href{https://aclanthology.org/2020.acl-main.463/}{Climbing towards NLU} (``Octopus Paper'')
  \item \href{https://aclanthology.org/2024.emnlp-main.651/}{Pragmatic Norms are All You Need} (Rebuttal to ``Octopus Paper'')
  \item \href{https://aclanthology.org/2023.blackboxnlp-1.21/}{Evaluating Transformer's Ability to Learn Mildly Context-Sensitive Languages}
  \item \href{https://www.sciencedirect.com/science/article/pii/S0010027719302495}{Ease of Learning Explain Semantic Universals}
\end{itemize}

Phonetics/Phonology (language sound structure)
\begin{itemize}
  \item \href{https://aclanthology.org/2024.mrl-1.16/}{Mitigating the Linguistic Gap with Phonemic Representations for Robust Cross-lingual Transfer}
  \item \href{http://individual.utoronto.ca/ewan_dunbar/pdfs/dunbar_synnaeve_dupoux_icphs_2015.pdf}{Quantitative Methods for Comparing Featural Representations}
  \item \href{https://openpublishing.library.umass.edu/scil/article/id/1080/}{Sound Analogies with Phoneme Embeddings}
  \item \href{https://aclanthology.org/2020.scil-1.36/}{Phonotactic learning with neural language models}
\end{itemize}

Morphology (word structure)
\begin{itemize}
  \item \href{https://aclanthology.org/N16-1077/}{Morphological Inflection Generation Using Character Sequence to Sequence Learning}
  \item \href{https://aclanthology.org/P16-2090/}{Single-Model Encoder-Decoder with Explicit Morphological Representation for Reinflection}
  \item \href{https://aclanthology.org/E17-2120/}{Neural Graphical Models over Strings for Principal Parts Morphological Paradigm Completion}
\end{itemize}

Syntax (sentence structure)
\begin{itemize}
  \item \href{https://aclanthology.org/N19-1419/}{A Structural Probe for Finding Syntax in Word Representations}
  \item \href{https://direct.mit.edu/tacl/article/doi/10.1162/tacl_a_00304/43542/Does-Syntax-Need-to-Grow-on-Trees-Sources-of}{Does Syntax Need to Grow on Trees?}
  \item \href{https://aclanthology.org/N16-1024/}{Recurrent Neural Network Grammars}
  \item \href{https://direct.mit.edu/tacl/article/doi/10.1162/tacl_a_00526/114315/Transformer-Grammars-Augmenting-Transformer}{Transformer Grammars}
\end{itemize}

Semantics (language meaning)
\begin{itemize}
  \item \href{https://aclanthology.org/P19-1009/}{AMR Parsing as Sequence-to-Graph Transduction}
  \item \href{https://direct.mit.edu/tacl/article/doi/10.1162/tacl_a_00396/106796/Joint-Universal-Syntactic-and-Semantic-Parsing}{Joint Universal Syntactic and Semantic Parsing}
\end{itemize}

\end{document}
